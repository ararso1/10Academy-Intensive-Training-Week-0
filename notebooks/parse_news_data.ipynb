{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Websites that have the largest count of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data_df = pd.read_csv('C:\\\\Users\\\\araso\\\\Documents\\\\10academy\\\\data\\\\data.csv')\n",
    "domains_location_df = pd.read_csv('C:\\\\Users\\\\araso\\\\Documents\\\\10academy\\\\data\\\\domains_location.csv')\n",
    "traffic_data_df = pd.read_csv('C:\\\\Users\\\\araso\\\\Documents\\\\10academy\\\\data\\\\traffic.csv')\n",
    "rating_df = pd.read_csv('C:\\\\Users\\\\araso\\\\Documents\\\\10academy\\\\data\\\\rating.csv')\n",
    "\n",
    "# Merge data.csv with domains_location.csv\n",
    "data_with_location = pd.merge(data_df, domains_location_df, left_on='source_name', right_on='SourceCommonName', how='left')\n",
    "\n",
    "# Merge the result with traffic_data.csv\n",
    "full_data = pd.merge(data_with_location, traffic_data_df, left_on='SourceCommonName', right_on='Domain', how='left')\n",
    "\n",
    "# 1. Websites that have the largest count of news articles\n",
    "article_counts = data_df['source_name'].value_counts()\n",
    "top_10_websites_articles = article_counts.head(10)\n",
    "bottom_10_websites_articles = article_counts.tail(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Websites by Article Count:\")\n",
    "print(top_10_websites_articles)\n",
    "print(\"\\nBottom 10 Websites by Article Count:\")\n",
    "print(bottom_10_websites_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Websites with the highest numbers of visitors traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Websites with the highest numbers of visitors traffic\n",
    "top_10_websites_traffic = traffic_data_df.sort_values(by='GlobalRank').head(10)[['Domain', 'GlobalRank']]\n",
    "bottom_10_websites_traffic = traffic_data_df.sort_values(by='GlobalRank').tail(10)[['Domain', 'GlobalRank']]\n",
    "\n",
    "print(\"\\nTop 10 Websites by Visitor Traffic:\")\n",
    "print(top_10_websites_traffic)\n",
    "print(\"\\nBottom 10 Websites by Visitor Traffic:\")\n",
    "print(bottom_10_websites_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Countries with the highest number of news media organisations (represented by domains in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Countries with the highest number of news media organisations (represented by domains in the data)\n",
    "country_domain_counts = domains_location_df['Country'].value_counts()\n",
    "top_10_countries_domains = country_domain_counts.head(10)\n",
    "bottom_10_countries_domains = country_domain_counts.tail(10)\n",
    "\n",
    "print(\"\\nTop 10 Countries by Number of Media Organisations:\")\n",
    "print(top_10_countries_domains)\n",
    "print(\"\\nBottom 10 Countries by Number of Media Organisations:\")\n",
    "print(bottom_10_countries_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Countries with Many Articles Written About Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Countries that have many articles written about them\n",
    "# We will check if the country names are mentioned in the article content\n",
    "\n",
    "# List of countries to check in the content\n",
    "countries_list = domains_location_df['Country'].unique()\n",
    "\n",
    "# Initialize a dictionary to count articles per country\n",
    "country_article_count = {country: 0 for country in countries_list}\n",
    "\n",
    "# Count occurrences of each country in the article content\n",
    "for country in countries_list:\n",
    "    country = str(country)\n",
    "    country_article_count[country] = data_df['content'].str.contains(country, case=False, na=False).sum()\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "country_article_count_df = pd.DataFrame(list(country_article_count.items()), columns=['Country', 'ArticleCount']).dropna()\n",
    "\n",
    "# Sort by ArticleCount to find the top and bottom 10 countries\n",
    "top_10_countries_articles = country_article_count_df.sort_values(by='ArticleCount', ascending=False).head(10)\n",
    "bottom_10_countries_articles = country_article_count_df.sort_values(by='ArticleCount', ascending=True).tail(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Countries by Article Count:\")\n",
    "print(top_10_countries_articles)\n",
    "\n",
    "print(\"\\nBottom 10 Countries by Article Count:\")\n",
    "print(bottom_10_countries_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Websites that reported about Africa, US, China, EU, Russia, Ukraine, Middle East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Websites that reported about Africa, US, China, EU, Russia, Ukraine, Middle East\n",
    "\n",
    "# Define country groups for each region\n",
    "african_countries = domains_location_df[domains_location_df['location'].isin(['DZ', 'AO', 'BJ', 'BW', 'BF', 'BI', 'CV', 'CM', 'CF', 'TD', 'KM', 'CG', 'CD', 'DJ', 'EG', 'GQ', 'ER', 'SZ', 'ET', 'GA', 'GM', 'GH', 'GN', 'GW', 'KE', 'LS', 'LR', 'LY', 'MG', 'MW', 'ML', 'MR', 'MU', 'YT', 'MA', 'MZ', 'NA', 'NE', 'NG', 'RE', 'RW', 'SH', 'ST', 'SN', 'SC', 'SL', 'SO', 'ZA', 'SS', 'SD', 'TZ', 'TG', 'TN', 'UG', 'EH', 'ZM', 'ZW'])]['SourceCommonName'].unique()\n",
    "\n",
    "eu_countries = domains_location_df[domains_location_df['location'].isin(['AT', 'BE', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FI', 'FR', 'DE', 'GR', 'HU', 'IE', 'IT', 'LV', 'LT', 'LU', 'MT', 'NL', 'PL', 'PT', 'RO', 'SK', 'SI', 'ES', 'SE'])]['SourceCommonName'].unique()\n",
    "\n",
    "middle_east_countries = domains_location_df[domains_location_df['location'].isin(['BH', 'EG', 'IR', 'IQ', 'IL', 'JO', 'KW', 'LB', 'OM', 'PS', 'QA', 'SA', 'SY', 'AE', 'YE'])]['SourceCommonName'].unique()\n",
    "\n",
    "# Initialize dictionaries for each region\n",
    "africa_article_count = 0\n",
    "us_article_count = 0\n",
    "china_article_count = 0\n",
    "eu_article_count = 0\n",
    "russia_article_count = 0\n",
    "ukraine_article_count = 0\n",
    "middle_east_article_count = 0\n",
    "\n",
    "# Check occurrences for each region in the article content\n",
    "africa_article_count = data_df['content'].apply(lambda x: any(country in x for country in african_countries)).sum()\n",
    "us_article_count = data_df['content'].str.contains('US', case=False, na=False).sum()\n",
    "china_article_count = data_df['content'].str.contains('China', case=False, na=False).sum()\n",
    "eu_article_count = data_df['content'].apply(lambda x: any(country in x for country in eu_countries)).sum()\n",
    "russia_article_count = data_df['content'].str.contains('Russia', case=False, na=False).sum()\n",
    "ukraine_article_count = data_df['content'].str.contains('Ukraine', case=False, na=False).sum()\n",
    "middle_east_article_count = data_df['content'].apply(lambda x: any(country in x for country in middle_east_countries)).sum()\n",
    "\n",
    "# Aggregate the article counts for each region\n",
    "region_article_count = {\n",
    "    'Africa': africa_article_count,\n",
    "    'US': us_article_count,\n",
    "    'China': china_article_count,\n",
    "    'EU': eu_article_count,\n",
    "    'Russia': russia_article_count,\n",
    "    'Ukraine': ukraine_article_count,\n",
    "    'Middle East': middle_east_article_count\n",
    "}\n",
    "\n",
    "region_article_count_df = pd.DataFrame(list(region_article_count.items()), columns=['Region', 'ArticleCount'])\n",
    "\n",
    "# Sort and display the top and bottom 10 websites for these regions\n",
    "top_10_region_articles = region_article_count_df.sort_values(by='ArticleCount', ascending=False).head(10)\n",
    "bottom_10_region_articles = region_article_count_df.sort_values(by='ArticleCount', ascending=True).head(10)\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Regions by Article Count:\")\n",
    "print(top_10_region_articles)\n",
    "\n",
    "print(\"\\nBottom 10 Regions by Article Count:\")\n",
    "print(bottom_10_region_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Websites with the highest count of positive, neutral, and negative sentiment? To do this you will need to group the data by website domain and apply descriptive statistics such as mean, median, and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert sentiment to numeric values for descriptive statistics\n",
    "sentiment_mapping = {'Negative': -1, 'Neutral': 0, 'Positive': 1}\n",
    "rating_df['title_sentiment_numeric'] = rating_df['title_sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Group by domain and calculate descriptive statistics\n",
    "sentiment_stats = rating_df.groupby('source_name')['title_sentiment_numeric'].agg(\n",
    "    mean_sentiment=np.mean,\n",
    "    median_sentiment=np.median,\n",
    "    variance_sentiment=np.var,\n",
    "    count='count'\n",
    ").reset_index()\n",
    "\n",
    "# Identify top 10 domains by visitor traffic\n",
    "top_10_traffic_domains = traffic_data_df.sort_values(by='GlobalRank').head(10)['Domain']\n",
    "\n",
    "# Filter sentiment statistics for these top 10 domains\n",
    "top_10_sentiment_stats = sentiment_stats[sentiment_stats['source_name'].isin(top_10_traffic_domains)]\n",
    "\n",
    "# Global sentiment distribution\n",
    "global_sentiment_distribution = rating_df['title_sentiment'].value_counts(normalize=True)\n",
    "\n",
    "# Sentiment distribution for top 10 domains\n",
    "top_10_domains_sentiment_distribution = rating_df[rating_df['source_name'].isin(top_10_traffic_domains)]['title_sentiment'].value_counts(normalize=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"Sentiment Statistics (Mean, Median, Variance) for Each Website Domain:\")\n",
    "print(sentiment_stats.sort_values(by='mean_sentiment', ascending=False))\n",
    "\n",
    "print(\"\\nComparison of Sentiment Distribution:\")\n",
    "print(\"Global Sentiment Distribution:\")\n",
    "print(global_sentiment_distribution)\n",
    "print(\"\\nTop 10 Domains Sentiment Distribution:\")\n",
    "print(top_10_domains_sentiment_distribution)\n",
    "\n",
    "print(\"\\nTop 10 Domains by Traffic with Sentiment Statistics:\")\n",
    "print(top_10_sentiment_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Similarity of Raw Message Lengths Across Sites and How similar are the number of words in the title across sites? Check the distribution among sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure that the title and full_content columns are treated as strings and handle missing values\n",
    "data_df['title'] = data_df['title'].astype(str)\n",
    "data_df['full_content'] = data_df['full_content'].astype(str)\n",
    "\n",
    "# Similarity of Raw Message Lengths Across Sites\n",
    "# Calculate the length of the full content for each article\n",
    "data_df['content_length'] = data_df['full_content'].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# Group by source_name to compare distributions\n",
    "content_length_distribution = data_df.groupby('source_name')['content_length'].describe()\n",
    "\n",
    "# Plot the distribution of content length for top websites\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='source_name', y='content_length', data=data_df, showfliers=False)\n",
    "plt.title('Distribution of Content Length Across Websites')\n",
    "plt.xlabel('Website')\n",
    "plt.ylabel('Content Length (Characters)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Similarity of the Number of Words in the Title Across Sites\n",
    "# Calculate the number of words in the title for each article\n",
    "data_df['title_word_count'] = data_df['title'].apply(lambda x: len(x.split()) if x else 0)\n",
    "\n",
    "# Group by source_name to compare distributions\n",
    "title_word_count_distribution = data_df.groupby('source_name')['title_word_count'].describe()\n",
    "\n",
    "# Plot the distribution of title word count for top websites\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='source_name', y='title_word_count', data=data_df, showfliers=False)\n",
    "plt.title('Distribution of Title Word Count Across Websites')\n",
    "plt.xlabel('Website')\n",
    "plt.ylabel('Title Word Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Display descriptive statistics for both analyses\n",
    "print(\"Descriptive Statistics of Content Length Across Sites:\")\n",
    "print(content_length_distribution)\n",
    "\n",
    "print(\"\\nDescriptive Statistics of Title Word Count Across Sites:\")\n",
    "print(title_word_count_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Do a 2D scatter plot where x-axis is the total number of reports by a website, y-axis is the global ranking of the site, and the color representing average/median sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the total number of reports by each website\n",
    "report_counts = rating_df.groupby('source_name').size().reset_index(name='total_reports')\n",
    "\n",
    "# Calculate the average and median sentiment for each website\n",
    "sentiment_agg = rating_df.groupby('source_name')['title_sentiment_numeric'].agg(\n",
    "    avg_sentiment='mean',\n",
    "    median_sentiment='median'\n",
    ").reset_index()\n",
    "\n",
    "# Merge with traffic data to get the global ranking\n",
    "merged_data = pd.merge(report_counts, sentiment_agg, on='source_name')\n",
    "merged_data = pd.merge(merged_data, traffic_data_df[['Domain', 'GlobalRank']], left_on='source_name', right_on='Domain')\n",
    "\n",
    "# Plot the 2D scatter plot with average sentiment\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    x=merged_data['total_reports'],\n",
    "    y=merged_data['GlobalRank'],\n",
    "    c=merged_data['avg_sentiment'],\n",
    "    cmap='coolwarm',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label='Average Sentiment')\n",
    "plt.xlabel('Total Number of Reports')\n",
    "plt.ylabel('Global Rank (Lower is Better)')\n",
    "plt.title('Impact of News Reporting and Sentiment on Global Ranking')\n",
    "plt.show()\n",
    "\n",
    "# Plot the 2D scatter plot with median sentiment\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    x=merged_data['total_reports'],\n",
    "    y=merged_data['GlobalRank'],\n",
    "    c=merged_data['median_sentiment'],\n",
    "    cmap='coolwarm',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label='Median Sentiment')\n",
    "plt.xlabel('Total Number of Reports')\n",
    "plt.ylabel('Global Rank (Lower is Better)')\n",
    "plt.title('Impact of News Reporting and Sentiment on Global Ranking')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perform Keyword extraction/modelling using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract top n keywords using TF-IDF\n",
    "def extract_keywords_tfidf(texts, n=5):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_keywords = []\n",
    "\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        tfidf_scores = tfidf_matrix[i].T.todense()\n",
    "        sorted_indices = tfidf_scores.argsort(axis=0)[::-1]\n",
    "        top_n = [feature_names[index[0, 0]] for index in sorted_indices[:n]]\n",
    "        top_keywords.append(top_n)\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "# Extract keywords for titles and full content\n",
    "data_df['title_keywords'] = extract_keywords_tfidf(data_df['title'].fillna(''), n=5)\n",
    "data_df['content_keywords'] = extract_keywords_tfidf(data_df['full_content'].fillna(''), n=5)\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity between two lists of keywords\n",
    "def calculate_similarity(keywords1, keywords2):\n",
    "    vectorizer = TfidfVectorizer().fit([' '.join(keywords1), ' '.join(keywords2)])\n",
    "    tfidf_matrix = vectorizer.transform([' '.join(keywords1), ' '.join(keywords2)])\n",
    "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "\n",
    "# Calculate similarity between title and content keywords across sites\n",
    "data_df['keyword_similarity'] = data_df.apply(lambda row: calculate_similarity(row['title_keywords'], row['content_keywords']), axis=1)\n",
    "\n",
    "# Analyze similarity by site\n",
    "similarity_by_site = data_df.groupby('source_name')['keyword_similarity'].mean().reset_index()\n",
    "\n",
    "# Display the results\n",
    "print(\"Similarity of Keywords in Titles and Content Across Sites:\")\n",
    "print(similarity_by_site.sort_values(by='keyword_similarity', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Fill missing values\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['full_content'] = df['full_content'].fillna('')\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['full_content'] = df['full_content'].astype(str)\n",
    "    df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid dates\n",
    "    df = df.dropna(subset=['published_at'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and clean data\n",
    "data_df = clean_data(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Perform topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 1 categorise the title/content into a known set of topic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.topic_modeling import *\n",
    "\n",
    "# Perform topic modeling\n",
    "n_topics = 10  # Number of topics to identify\n",
    "lda, topics, vectorizer = perform_topic_modeling_by_lda(data_df, n_topics=n_topics)\n",
    "\n",
    "# Assign the most likely topic to each article\n",
    "topic_assignments = lda.transform(vectorizer.transform(data_df['text'])).argmax(axis=1)\n",
    "data_df['topic'] = topic_assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 2 Which websites reported the most diverse topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_topic_diversity(df):\n",
    "    # Calculate the number of unique topics reported by each website\n",
    "    diversity = df.groupby('source_name')['topic'].nunique().reset_index()\n",
    "    diversity.columns = ['source_name', 'unique_topics']\n",
    "    \n",
    "    # Sort by most diverse topics\n",
    "    most_diverse_sites = diversity.sort_values(by='unique_topics', ascending=False)\n",
    "    \n",
    "    return most_diverse_sites\n",
    "\n",
    "# Analyze topic diversity\n",
    "most_diverse_sites = analyze_topic_diversity(data_df)\n",
    "print(\"Websites Reporting the Most Diverse Topics:\")\n",
    "print(most_diverse_sites.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 3 Analyse the topic trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_topic_trends(df, n_topics):\n",
    "    # Group by date and topic, then count the occurrences\n",
    "    trend_data = df.groupby([df['published_at'].dt.date, 'topic']).size().reset_index(name='count')\n",
    "    \n",
    "    # Plot the trends\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = plt.gca()  # Get the current axis\n",
    "    scatter = sns.scatterplot(data=trend_data, x='published_at', y='topic', size='count', hue='count', palette='viridis', sizes=(20, 200), legend=False, ax=ax)\n",
    "\n",
    "    # Manually create the colorbar\n",
    "    norm = plt.Normalize(trend_data['count'].min(), trend_data['count'].max())\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "    sm.set_array([])\n",
    "    \n",
    "    # Add the colorbar to the correct axis\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Number of Articles')\n",
    "    \n",
    "    plt.title('Topic Trends Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Topic')\n",
    "    plt.yticks(range(n_topics), labels=[f'Topic {i+1}' for i in range(n_topics)])\n",
    "    plt.show()\n",
    "\n",
    "# Plot topic trends\n",
    "plot_topic_trends(data_df, n_topics=n_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Model the events that the news articles are written about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Clean text data and combine title and content\n",
    "    df['title'] = df['title'].fillna('').apply(clean_text)\n",
    "    df['full_content'] = df['full_content'].fillna('').apply(clean_text)\n",
    "    df['text'] = df['title'] + \" \" + df['full_content']\n",
    "    return df\n",
    "\n",
    "# Load and prepare data\n",
    "data_df = prepare_data(data_df)\n",
    "\n",
    "def vectorize_text(df):\n",
    "    # Fill NaN values with empty strings to avoid errors during vectorization\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    \n",
    "    # Use TF-IDF to vectorize the text data\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english', max_features=10000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "tfidf_matrix, vectorizer = vectorize_text(data_df)\n",
    "\n",
    "def cluster_texts(tfidf_matrix, num_clusters=10):\n",
    "    # Apply K-Means clustering\n",
    "    km = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    return km, clusters\n",
    "\n",
    "# Determine the number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Perform clustering\n",
    "kmeans_model, clusters = cluster_texts(tfidf_matrix, num_clusters)\n",
    "data_df['cluster'] = clusters\n",
    "\n",
    "def get_top_keywords(cluster_centers, vectorizer, top_n=10):\n",
    "    keywords = []\n",
    "    for i, cluster_center in enumerate(cluster_centers):\n",
    "        top_indices = cluster_center.argsort()[-top_n:]\n",
    "        top_keywords = [vectorizer.get_feature_names_out()[index] for index in top_indices]\n",
    "        keywords.append(top_keywords)\n",
    "    return keywords\n",
    "\n",
    "# Get top keywords for each cluster\n",
    "top_keywords = get_top_keywords(kmeans_model.cluster_centers_, vectorizer, top_n=10)\n",
    "\n",
    "# Assign event descriptions based on top keywords\n",
    "data_df['event_description'] = data_df['cluster'].apply(lambda x: ', '.join(top_keywords[x]))\n",
    "\n",
    "# Display the event descriptions\n",
    "for i, keywords in enumerate(top_keywords):\n",
    "    print(f\"Event {i+1}: {', '.join(keywords)}\")\n",
    "\n",
    "# Analyze the number of articles and media covering each event\n",
    "event_analysis = data_df.groupby(['cluster', 'event_description']).agg(\n",
    "    num_articles=('article_id', 'count'),\n",
    "    num_media=('source_name', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Display the event analysis\n",
    "print(\"Event Analysis:\")\n",
    "print(event_analysis.sort_values(by='num_articles', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Clean and prepare the data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "data_df['title'] = data_df['title'].apply(clean_text)\n",
    "data_df['full_content'] = data_df['full_content'].apply(clean_text)\n",
    "data_df['text'] = data_df['title'] + \" \" + data_df['full_content']\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english', max_features=10000)\n",
    "tfidf_matrix = vectorizer.fit_transform(data_df['text'])\n",
    "\n",
    "# Apply K-Means clustering to group articles into events\n",
    "num_clusters = 10  # Adjust the number of clusters based on the dataset size\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "data_df['event_cluster'] = kmeans.labels_\n",
    "\n",
    "# 1. How many events are covered in the data?\n",
    "num_events = data_df['event_cluster'].nunique()\n",
    "print(f\"Number of events covered in the data: {num_events}\")\n",
    "\n",
    "# 2. Which news sites report events the earliest?\n",
    "data_df['published_at'] = pd.to_datetime(data_df['published_at'], errors='coerce')\n",
    "earliest_reports = data_df.groupby(['event_cluster', 'source_name'])['published_at'].min().reset_index()\n",
    "earliest_site_per_event = earliest_reports.loc[earliest_reports.groupby('event_cluster')['published_at'].idxmin()]\n",
    "print(\"Earliest reporting news sites per event:\")\n",
    "print(earliest_site_per_event)\n",
    "\n",
    "# 3. Which events have the highest reporting?\n",
    "event_reporting = data_df['event_cluster'].value_counts().reset_index()\n",
    "event_reporting.columns = ['event_cluster', 'num_articles']\n",
    "print(\"Events with the highest reporting:\")\n",
    "print(event_reporting.head(10))\n",
    "\n",
    "# 4. Correlation between news sites reporting events\n",
    "# Create a pivot table to count the number of reports per site per event\n",
    "pivot_table = pd.pivot_table(data_df, index='event_cluster', columns='source_name', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Calculate the correlation between news sites based on events they report\n",
    "correlation_matrix = pivot_table.corr()\n",
    "print(\"Correlation matrix between news sites reporting events:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Version your ML models and their artefacts using MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 1 Train and Log the Topic Modeling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set the MLFlow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Change this if you're using a different tracking server\n",
    "mlflow.set_experiment(\"News_Article_Models\")\n",
    "\n",
    "# Assuming tfidf_matrix is already prepared\n",
    "\n",
    "with mlflow.start_run(run_name=\"LDA_Topic_Modeling\"):\n",
    "    # Train the LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "    lda.fit(tfidf_matrix)\n",
    "    \n",
    "    # Log parameters and metrics (example: log the number of components)\n",
    "    mlflow.log_param(\"n_components\", 10)\n",
    "    \n",
    "    # Log the LDA model\n",
    "    mlflow.sklearn.log_model(lda, \"LDA_model\")\n",
    "    \n",
    "    # Optionally log the topics discovered by the model\n",
    "    topics = get_top_keywords(lda.components_, vectorizer, top_n=10)\n",
    "    for i, topic in enumerate(topics):\n",
    "        mlflow.log_text(f\"Topic {i+1}: {', '.join(topic)}\", f\"topic_{i+1}.txt\")\n",
    "    \n",
    "    print(\"LDA model and topics logged in MLFlow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 2 Train and Log the Event Modeling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming tfidf_matrix and other necessary data are prepared\n",
    "\n",
    "with mlflow.start_run(run_name=\"KMeans_Event_Clustering\"):\n",
    "    # Train the K-Means model\n",
    "    num_clusters = 10\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_clusters\", num_clusters)\n",
    "    \n",
    "    # Log the K-Means model\n",
    "    mlflow.sklearn.log_model(kmeans, \"KMeans_model\")\n",
    "    \n",
    "    # Log the cluster centers or other relevant artifacts\n",
    "    for i, center in enumerate(kmeans.cluster_centers_):\n",
    "        mlflow.log_text(f\"Cluster {i+1} center: {center}\", f\"cluster_{i+1}_center.txt\")\n",
    "    \n",
    "    print(\"K-Means model and cluster centers logged in MLFlow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'db'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      9\u001b[0m Base\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcreate_all(bind\u001b[38;5;241m=\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\araso\\Documents\\10academy\\Week-0\\notebooks\\..\\src\\db\\models.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, String, Integer, Text, TIMESTAMP, ForeignKey\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# import sys\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# sys.path.append('../')\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Base\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mArticle\u001b[39;00m(Base):\n\u001b[0;32m      8\u001b[0m     __tablename__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'db'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.data_loader import *\n",
    "from src.db.models import *\n",
    "from src.db.db_config import *\n",
    "\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "print(data_df.head())\n",
    "# Function to load data into the database\n",
    "def load_data(data_df):\n",
    "    session = SessionLocal()\n",
    "\n",
    "    # Convert data to dictionaries for insertion\n",
    "    articles_data = data_df[['article_id', 'source_name', 'published_at', 'title', 'full_content']].to_dict(orient='records')\n",
    "    topics_data = [{'topic_keywords': ', '.join(topics[i]), 'article_id': row['article_id']} for i, row in data_df.iterrows()]\n",
    "    events_data = [{'event_description': row['event_description'], 'article_id': row['article_id']} for i, row in data_df.iterrows()]\n",
    "    features_data = [{'article_id': row['article_id'], 'tfidf_vector': str(tfidf_matrix[i].toarray()), 'topic': row['topic'], 'event_cluster': row['event_cluster']} for i, row in data_df.iterrows()]\n",
    "\n",
    "    try:\n",
    "        # Insert data into tables\n",
    "        session.bulk_insert_mappings(Article, articles_data)\n",
    "        session.bulk_insert_mappings(Topic, topics_data)\n",
    "        session.bulk_insert_mappings(Event, events_data)\n",
    "        session.bulk_insert_mappings(Feature, features_data)\n",
    "        session.commit()\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "load_data(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
